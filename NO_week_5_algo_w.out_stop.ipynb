{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from case_studies import *\n",
    "from scipy.optimize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_track(f,df,x,p,a=1,c1=0.01,rho=0.5):\n",
    "    while f(np.array(x+a*p)) > f(np.array(x)) + c1*a* p.T@df(np.array(x)): #Check if a fulfills sufficient decrease conditions\n",
    "        a = a*rho #If not, we decrease a by a factor rho\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_newton(x0, A, f, df, hf,iterations=10):\n",
    "    \"\"\"\n",
    "    Estimates the local minimum of f using the Newton algorithm with equality constarints\n",
    "    \"\"\"\n",
    "    x_k = x0\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Check if hessian is positive definit\n",
    "        if np.all(np.linalg.eigvals(hf(x_k)) > 0):\n",
    "            B = hf(x_k)\n",
    "\n",
    "        else:\n",
    "            #compute approx hessian\n",
    "            values,vectors = np.linalg.eig(hf(x_k))\n",
    "            for i in range(len(values)):\n",
    "                B =+ np.abs(values[i]) * np.outer(vectors[i],vectors[i])\n",
    "\n",
    "        #create the linear system\n",
    "        matrix_A = np.block([[B, A.T], [A, np.zeros([A.shape[0],A.shape[0]])]])\n",
    " \n",
    "        matrix_B = np.block([-df(x_k),np.zeros(A.shape[0])])\n",
    "\n",
    "        #solve the linear system and extract pk\n",
    "        solved = np.linalg.solve(matrix_A, matrix_B)\n",
    "\n",
    "        p_k = solved[:A.shape[1]]\n",
    "\n",
    "        lam = solved[A.shape[1:]]\n",
    "\n",
    "        #if np.linalg.norm(df(x_k)+A*lam) < 10**(-10): #stopping criteria \n",
    "            #break\n",
    "\n",
    "        # Compute the step size\n",
    "        alpha_k = back_track(x=x_k, p=p_k, f=f, df=df)\n",
    "\n",
    "        # Update the current solution\n",
    "        x_k = x_k + alpha_k*p_k\n",
    "\n",
    "    return -x_k,i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2\n",
    "n = 3\n",
    "\n",
    "A = np.random.rand(m,n)\n",
    "x = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "x0 = x - np.linalg.pinv(A) @ (A@x + b)\n",
    "\n",
    "x_newton,iterations_newton = constrained_newton(x0=np.array(x0),A=A,f=f3,df=df3,hf=Hf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_steepest_descent(f,df,x,A,max_iterations=10000,rho=0.5):\n",
    "    \"\"\"\n",
    "    f is function that is being optimized\n",
    "    df is the gradient of f\n",
    "    x is staring point\n",
    "    \"\"\"\n",
    "    beta = 1\n",
    "    M = np.identity(A.shape[1]) - A.T @ np.linalg.inv(A@A.T) @ A\n",
    "    for i in range(0,max_iterations):\n",
    "        pk = -M @ df(x) #we set the ray direction\n",
    "\n",
    "        ak = back_track(f=f,df=df,x=x,p=pk,a=beta) #We find the step size\n",
    "\n",
    "        x = x + ak*pk #we update x with the stepsize and direction\n",
    "\n",
    "        lam = np.linalg.norm(np.linalg.inv(A@A.T)@A@df(x)) #we calculate lambda with the eq we found in theory\n",
    "\n",
    "\n",
    "        #print(\"stop\",np.linalg.norm(df(x)+np.inner(A,lam)))\n",
    "        if np.linalg.norm(df(x)+np.inner(A,lam)) < 10**(-5) :\n",
    "            break\n",
    "    \n",
    "        beta = ak/rho #We calculate the new initial step size\n",
    "\n",
    "    return(-x,i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1\n",
    "n = 3\n",
    "\n",
    "A = np.random.rand(m,n)\n",
    "x = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "x0 = x - np.linalg.pinv(A) @ (A@x + b)\n",
    "\n",
    "x,iterations = constrained_steepest_descent(f=f3,df=df3,x=np.array(x0),A=A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton's algorithm:\n",
      " iterations: 20 point found: [-1.76376637e-01 -5.10623702e-01  6.81114083e-01 -9.85285213e-02\n",
      " -1.31120493e-01  4.42928369e-01 -1.27398499e-01 -3.47497363e-01\n",
      "  3.42520283e-01 -7.15839225e-01 -2.64267277e-02  4.89363994e-01\n",
      "  2.22871849e-01  3.26779612e-01  3.49151614e-01 -2.43140084e-05\n",
      " -2.22133194e-01  4.17676494e-01  4.04078016e-02  1.91865316e-01]\n",
      "Steepest descent algorithm:\n",
      " iterations: 10000 point found: [-1.76388265e-01 -5.10511824e-01  6.81115220e-01 -9.85426320e-02\n",
      " -1.31181099e-01  4.42939955e-01 -1.27362399e-01 -3.47510600e-01\n",
      "  3.42546117e-01 -7.15800678e-01 -2.64587822e-02  4.89371217e-01\n",
      "  2.22793717e-01  3.26802212e-01  3.49112274e-01 -3.32421407e-05\n",
      " -2.22140613e-01  4.17608742e-01  4.04675718e-02  1.91941030e-01]\n"
     ]
    }
   ],
   "source": [
    "m = 19\n",
    "n = 20\n",
    "\n",
    "A = np.random.rand(m,n)\n",
    "x = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "x0 = x - np.linalg.pinv(A) @ (A@x + b)\n",
    "\n",
    "x_newton,iterations_newton = constrained_newton(x0=np.array(x0),A=A,f=f3,df=df3,hf=Hf3)\n",
    "x_sd,iterations_sd = constrained_steepest_descent(f=f3,df=df3,x=np.array(x0),A=A)\n",
    "\n",
    "print(\"Newton's algorithm:\\n iterations:\",iterations_newton,\"point found:\",x_newton)\n",
    "\n",
    "print(\"Steepest descent algorithm:\\n iterations:\",iterations_sd,\"point found:\",x_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.76391052e-01 -5.10485008e-01  6.81115492e-01 -9.85460140e-02\n",
      " -1.31195626e-01  4.42942732e-01 -1.27353747e-01 -3.47513772e-01\n",
      "  3.42552309e-01 -7.15791438e-01 -2.64664650e-02  4.89372948e-01\n",
      "  2.22774989e-01  3.26807629e-01  3.49102844e-01 -3.53821462e-05\n",
      " -2.22142391e-01  4.17592502e-01  4.04818979e-02  1.91959177e-01]\n"
     ]
    }
   ],
   "source": [
    "linear_constraints = {\"type\": \"eq\", \"fun\": lambda x: A.dot(x) - b}\n",
    "\n",
    "opt = minimize(f3,x0=x0,constraints=linear_constraints)\n",
    "print(opt.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77dfb7e60ff99a60e9a4687749d992fb0ceb80ae5da03293ead0050a18338808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
